---
title: "CLAD & CUDA Simple Tutorial"
layout: textlay
excerpt: CLAD & CUDA Simple Tutorial"
sitemap: false
permalink: /tutorials/clad_cuda_simple/
---

**AD Tutorial - CUDA Deploy Your CLAD Custom Function**

*Tutorial level: Intro*

CLAD provides automatic differentiation (AD) for C/C++ and works without code 
modification (legacy code). Given that the range of AD application problems are 
defined by their high computational requirements, it means that they can greatly 
benefit from parallel implementations on graphics processing units (GPUs).

This tutorial showcases how to firstly use CLAD to obtain your function’s 
gradient and how to schedule the function’s execution on the GPU. 

**Definition of the custom function**

For the purpose of this tutorial, the custom function chosen is the Gauss 
Function. We include Clad and we defined the function such as: 


```
#include <iostream>
#include "clad/Differentiator/Differentiator.h"

#define N 10000

__device__ __host__ double gauss(double* x, double* p, double sigma, int dim) {
    double t = 0;
    for (int i = 0; i< dim; i++)
        t += (x[i] - p[i]) * (x[i] - p[i]);
    t = -t / (2*sigma*sigma);
    return std::pow(2*M_PI, -dim/2.0) * std::pow(sigma, -0.5) * std::exp(t);
}
```


**Definition of Clad gradient**

Having our custom function declared, we now forward declare our AD function 
following Clad’s convention, call Clad gradient and set a device function 
pointer to be used for the GPU execution:


```
typedef void(*func) (double* x, double* p, double sigma, int dim, double* _result);

//Body to be generated by Clad
__device__ __host__ gauss_grad(double* x, double* p, double sigma, int dim, double* _result); 

auto gauss_g = clad::gradient(gauss);

//Device function pointer
__device__ func p_gauss = gauss_grad;
```


**Definition of CUDA kernel**

The CUDA support for Clad includes extensions that allow one to execute functions 
on the GPU using many threads in parallel. Thus the kernel for AD execution is to 
be defined as: 


```
__global__ void compute(func op, double* d_x, double* d_y,
                        double* d_sigma, int n, double* result_dx, 
                        double* result_dy) {
    int i = blockIdx.x*blockDim.x + threadIdx.x;
    if (i < N) {
        double result_dim[3] = {};
        (*op)(&d_x[i],&d_y[i], &d_sigma, 1, result_dim);
        result_dx[i] = result_dim[0];
        result_dy[i] = result_dim[1];
    }
}
```


**Defining variables and stating execution**

The next and final step implies the definition of host variables and the copying 
both of host variables to the device but also of device results back to the host. 


```
int main(void) {
   double *x, *d_x;
   double *y, *d_y;
   double *sigma, *d_sigma;
   x = (double*)malloc(N*sizeof(double));
   y = (double*)malloc(N*sizeof(double));
   sigma = 0.2;
   for (int i = 0; i < N; i++) {
       x[i] = rand()%100;
       y[i] = rand()%100;
   }

   func h_gauss;

   cudaMalloc(&d_x, N*sizeof(double));
   cudaMemcpy(d_x, x, N*sizeof(double), cudaMemcpyHostToDevice);
   cudaMalloc(&d_y, N*sizeof(double));
   cudaMemcpy(d_y, y, N*sizeof(double), cudaMemcpyHostToDevice);
   cudaMalloc(&d_sigma, sizeof(double));
   cudaMemcpy(d_sigma, sigma, sizeof(double), cudaMemcpyHostToDevice);

   double *dx_result, *dy_result;
   cudaMalloc(&dx_result, N*sizeof(double));
   cudaMalloc(&dy_result, N*sizeof(double));
   double *result_x, *result_y;
   result_x = (double*)malloc(N*sizeof(double));
   result_y = (double*)malloc(N*sizeof(double));

   cudaMemcpyFromSymbol(&h_gauss, p_gauss, sizeof(func));
   compute<<<N/256+1, 256>>>(h_gauss, d_x, d_y, d_sigma, N, dx_result, dy_result);
   cudaDeviceSynchronize();
   cudaMemcpy(result_x, dx_result, N*sizeof(double), cudaMemcpyDeviceToHost);
   cudaMemcpy(result_y, dy_result, N*sizeof(double), cudaMemcpyDeviceToHost);
}
```

**Benchmarking GPU Accelerated Clad**

Following the implementation pattern described in this tutorial, will bring 
forth an increase in performance. The execution time for the GPU implementation 
decreases with the increase of dimensions, firstly in the case of the above 
Gauss function and secondly in the case of a product function gradient: 

<div align=center style="max-width:1095px; margin:0 auto;">
  <img src="{{ site.url }}{{ site.baseurl }}/images/tutorials/clad_cuda_simple/clad_cuda_simple1.png" 
  style="max-width:90%;"><br/>
 <p align="center">
  </p>
</div>